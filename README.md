# Real-time ASL Detection with YOLOv5
Communication is a fundamental human right, yet barriers exist for ASL users. Our project aims to bridge this gap, enabling real-time ASL interpretation using the YOLOv5 algorithm.

## Introduction
ASL serves the Deaf and Hard of Hearing community but lacks accessible tools for broader communication. Our team, Sreekutty, Gloria, and Krishna Haritha, developed a groundbreaking project using machine learning and computer vision.

## Objectives
- **Real-time Gesture Detection:** Detect and track ASL gestures using the front camera.
- **Gesture Classification:** Classify and interpret detected gestures.
- **User-Friendly Interface:** Present ASL interpretations intuitively.
- **Instant Feedback:** Enable fluid conversations with immediate responses.
- **Accessibility:** Serve both learners and fluent signers.

## Scope
- **ASL Alphabet:** Focus on detecting ASL alphabet gestures.
- **Real-time Detection:** Operates on various platforms with real-time video input.
- **Cross-Platform:** Developed in Python for desktop.

## Conclusion
Our project empowers ASL users to communicate inclusively, leveraging technology to bridge gaps. It's not just a machine learning project; it's a step toward a more inclusive world.

## Key Achievements:
- **Empowering Communication:** Bridging the ASL communication gap.
- **Accessibility:** Intuitive for both learners and fluent signers.
- **Technical Innovation:** YOLOv5 for real-time gesture detection.
- **Future Possibilities:** Expanding vocabulary and wider deployment.
